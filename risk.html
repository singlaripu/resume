<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame
        Remove this if you use the .htaccess -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Fico Interview Notes</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1;">
    <link href="http://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css" rel="stylesheet">
    <link href="http://netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.min.css" rel="stylesheet">
    <link href="./static/style1.css" rel="stylesheet" type="text/css">
    <link href="./static/style2.css" rel="stylesheet" type="text/css">
    <style type="text/css" media="screen">

        body, h1, h2, h3, h4, h5 {
            font-family:'Open Sans', sans-serif;
        }
        .name {
            font-family:'Josefin Sans', sans-serif;
            font-size:3em;
            line-height:1.5em;
        }
        .title {
            margin-top:0;
            /*color:#456A8F;*/
        }
        .header-row, .date, .jobdetails {
            color:#666;
        }
        .cattitle, .navbar-brand {
            /*color:#456A8F !important;*/
        }
        @media (max-width: 500px) {
            .name {
                font-size: 2em;
            }
            .cattitle {
                font-size: 1.6em;
            }
        }
        .dl-horizontal dt {
            text-align: left;
            width: 100px;
        }
        .dl-horizontal dd {
            margin-left: 110px;
        }
    </style>
</head>

<body>


<div class="container">

    <h1 class="page-header">
        <strong> My Notes on "Credit Risk Scorecards" by  NAEEM SIDDIQI </strong>
    </h1>


    <h3 class="page-header"> People's Role / Responsibilities</h3>

    <h4 class="bs-example">Credit Risk Manager</h4>
    <ol>
        <li>Data elements inputs</li>
        <li>Historical changes in Market/Portfolio</li>
    </ol><br>

    <h4 class="bs-example">Product Manager</h4>
    <ol>
        <li>Company's typical client base / target markets</li>
        <li>Future product development / marketing direction</li>
        <li>can assist in segmentation</li>
        <li>impact of strategies</li>
    </ol><br>

    <p><mark>Segmentation assess risk for increasingly specific population
        <icon class="glyphicon glyphicon-arrow-right"></icon>
        Involvement of marketing can help </mark>
    </p><br>

    <h4>Adjudication / Collection & Fraud</h4>
    <ol>
        <li>can help with experience based insights into factors that are predictive of negative behavior</li>
        <li>Question: what characteristics do you see in bad accounts? Have they changed over last few years?</li>
    </ol>



    <br><h3 class="page-header"> Score Card Development process - Overview</h3>

    <h4 class="bs-example"><strong>Stage 1: Planning</strong></h4>
    <ol>
        <li>Business Plan</li>
        <li>Org Objective / Score card Role</li>
        <li>Project Plan</li>
        <li> Project team and Responsibilities</li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 2: Data Review & Project parameters </strong></h4>
    <ol>
        <li> Data Availability and Quality </li>
        <li> Data gathering </li>
        <li> Sample and Performance window </li>
        <li> Target Definition </li>
        <li> Exclusions </li>
        <li> Segmentation </li>
        <li> Methodology </li>
        <li> Implementation Plan </li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 3: Dev Data Creation </strong></h4>
    <ol>
        <li> Dev sample specifications </li>
        <li> Sampling </li>
        <li> Dev data collection / Construction </li>
        <li> <code>Weights / Bias ??</code> </li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 4: Scorecard Development </strong></h4>
    <ol>
        <li> Exploring Data </li>
        <li> Identifying Missing values and Outliers </li>
        <li> Correlation </li>
        <li> Initial Characteristic Analysis - Fineclass, IV etc. </li>
        <li> Preliminary Scorecard </li>
        <li> Reject Inference </li>
        <li> Final Scorecard </li>
        <li> Scaling </li>
        <li>Validation</li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 5: Reports </strong></h4>
    <ol>
        <li> Gains Tables </li>
        <li> Characteristics Reports </li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 6: Implementation </strong></h4>
    <ol>
        <li> Scoring Strategy </li>
        <li> Setting Cutoffs </li>
        <li> Policy Rules / Adverse Action </li>
        <li> Overrides </li>
    </ol><br>


    <h4 class="bs-example"><strong>Stage 7: Post Implementation </strong></h4>
    <ol>
        <li> Scorecard performance reports </li>
        <li> Portfolio performance reports </li>
    </ol>


    <br><h3 class="page-header"> Stage 1 - Details</h3>

    <h4 class="bs-example"><strong> Stage 1 > Planing > Business Plan </strong></h4>
    <ol>
        <li> Increase Revenue  (Low Cutoff) <mark> vs </mark> Reduce Losses (High Cutoff) </li>
        <li> Reduction in Bad Debt / Bankruptcy / Claims / Fraud </li>
        <li> Increase in Approval Rates or Market Share </li>
        <li> Increase Profitability </li>
        <li><code> Increase Operational Efficiency ?? </code></li>
        <li> Better predictive power </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 1 > Planing > Scorecard Role </strong></h4>
    <ol>
        <li> Sole Decision maker <mark> vs </mark> support tool </li>
        <li> All possible information / variables in sole arbiter <mark> vs </mark> only complementary variables in support tool </li>
    </ol><br>


    <p ><mark> Sole Arbiter example
        <icon class="glyphicon glyphicon-arrow-right"></icon>
        Top 3% - selected applicants routed to a credit analyst </mark>
    </p>

    <p><mark> Expert Systems / Experience based models
        <icon class="glyphicon glyphicon-arrow-right"></icon>
        when Low Volumes, Cost / profit dynamics don't work for a full fledged score card development </mark>
    </p>


    <br><h3 class="page-header"> Stage 2 - Details</h3>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Data Gathering > Variables</strong></h4>
    <ol>
        <li> Account id </li>
        <li> Date Opened / MOB </li>
        <li> claim history over life of account </li>
        <li> Accept / Reject Indicator </li>
        <li> Product / Channel / Segment </li>
        <li> Current account status - Inactive / Closed / Lost / Stolen / Fraud etc. </li>
        <li> Demographics - Age, Geography, Existing Relationship with Bank, Marketing Campaigns (Customer Acq) </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Exclusions </strong></h4>
    <ol>
        <li> Accounts that would be scored - should be used for development </li>
        <li> Accounts that would be chucked out using any non-score criterion should not be part of dev sample. For example
            <ul>
                <li> VIP </li>
                <li> Staff </li>
                <li> Out of country </li>
                <li> Pre approved </li>
                <li> lost / stolen cards </li>
                <li> deceased </li>
                <li> under age </li>
                <li><code> voluntary cancellation within performance window ?? </code></li>
                <li> Non operating Market / Area </li>
            </ul>
        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Performance Window Selection </strong></h4>
    <ol>
        <li> Plot Bad Rate <mark> vs </mark> Months on Book where curve gets stable </li>
        <li> Reasons:
            <ul>
                <li> To avoid understating the bad rate </li>
                <li> Give enough time for cohort to mature </li>
                <li> Correctly label all the bads </li>
            </ul>
        </li>
        <li> Typical Performance window by product :
            <ul>
                <li> Credit Cards Acquisition - 18 to 24 months </li>
                <li> Mortgage - 3 to 5 years </li>
                <li> Behavior score - 6 or 12 months </li>
                <li> Collection models - 2 weeks or 1 month (Performance window as per Basel II guidelines) </li>
            </ul>
        </li>
    </ol><br>

    <p>
        <mark>
            Credit card is higher risk product than Mortgage hence shorter performance window.
        </mark>
    </p><br>

    <p>
        <mark>
            Example: Customer in distress would default first on credit card than Mortgage
        </mark>
    </p><br>

    <p>
        <mark>
            30 days delinquent will mature faster than 90 days delinquent.
        </mark>
    </p><br>

    <p>
        <mark>
            "Ever Bad" definition should be tried wherever possible.
        </mark>
    </p><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Sample Window > Seasonality </strong></h4>
    <ol>
        <li> Investigate the reason of abnormality  / Bias - e.g. Marketing Campaign</li>
        <li> Exclude abnormal population - if future target population do not contain this segment </li>
        <li> Try multiple cohorts - Monthly / Quarterly </li>
        <li> Apply weights to un-bias </li>
        <li> Exclude abnormal accounts - after some analysis / checks on discarded accounts </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Target Definition </strong></h4>
    <ol>
        <li> Org Objective
            <ul>
                <li> Profits - Delinquency point for non profitability </li>
                <li> Detection - Ever Bad </li>
                <li> Claims - Claims over $1,000 </li>
                <li> Collection - Less than 50% recovered within 3 months </li>
            </ul>
        </li><br>

        <li> Definition Strength
            <ul>
                <li> Tighter / Stringent Definition - Write-off / 120 DPD
                    <ul>
                        <li> Low Sample Size </li>
                        <li> Extreme / Higher Differentiation </li>
                    </ul>
                </li>

                <li> Looser Definition - 30 DPD
                    <ul>
                        <li> High Sample Size </li>
                        <li> Low Differentiation </li>
                        <li><mark> Weak Scorecard </mark></li>
                    </ul>
                </li>
            </ul>
        </li><br>

        <li> Interpretable & Trackable - Bad Example: 3x30 DPD or 2x60 DPD or 1x90 DPD or worse </li>
        <li> Depending upon accounting policies of write off </li>
        <li> Based on Existing score cards across segments </li>
        <li> Depending on regulatory environment </li>
        <li> Depending on Data availability
            <ul>
                <li> 12 months only data - 30 DPD </li>
                <li> Monthly payment data not available - "Current status" as compared to "EVER BAD" </li>
            </ul>
        </li>
    </ol><br>

    <p>
        <mark>
            BASEL II Guidelines <icon class="glyphicon glyphicon-arrow-right"></icon> 90 DPD
        </mark>
    </p><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Target Definition > Confirming BAD Definition </strong></h4>
    <ol>
        <li> Consensus Method </li>
        <li>
            Roll Rate Analysis - worst delinquency in "previous x months" compared to worst delinquency in "next x months"
            and then calculate % of accounts that maintain / roll forward / backward.
        </li>
        <li> Current <mark> vs </mark> Worst delinquency comparison </li>
    </ol><br>

    <p>
        <mark>
            BASEL II defines <strong> Default </strong> as any point at which the bank considers the obligor unlikely
            to repay the debt in full.
        </mark>
    </p><br>

    <h4 class="bs-example"><strong> Stage 2 > Data Review > Target Definition > Good Accounts </strong></h4>
    <ol>
        <li> Never delinquent or delinquent to a point where forward roll rate is less than 10% </li>
        <li> profitable or +ve NPV </li>
        <li> No claims </li>
        <li> Never bankrupt </li>
        <li> No frauds </li>
    </ol><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Target Definition > Indeterminates </strong></h4>
    <ol>
        <li> Can not be classified as either Good or Bad </li>
        <li> Insufficient performance history </li>
        <li> Mild delinquency with roll rates neither low enough to be Good nor High enough to be Bad </li>
        <li> 30 or 60 DPD accounts that do not roll forward </li>
        <li> Inactive, voluntarily cancelled, offer declined or applications approved but not booked accounts  </li>
        <li> Accounts with insufficient usage - for example, accounts with "High Balance" less than $20. </li>
        <li> Insurance accounts with low claims </li>
        <li> Accounts with NPV = 0 </li>
    </ol><br>


    <p>
        <mark>
            Indeterminates are required when "BAD" definition can be established several ways and usually not required
            when "BAD" definition is clear cut, e.g. Bankruptcy.
        </mark>
    </p><br>

    <p>
        <mark>
            Indeterminates should not exceed more than 10% to 15% of portfolio.
        </mark>
    </p><br>

    <p>
        <mark>
            Only Accounts that are defined as "Good" or "Bad" and rejects are used in actual development of scorecard.
        </mark>
    </p><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Segmentation </strong></h4>
    <ol>
        <li> When Portfolio is made of distinct sub populations and one scorecard would not work efficiently for all of them.  </li>
        <li> Segmentation based on experience and industry knowledge and then validating </li>
        <li> Segmentation based on statistical techniques such as Clustering & Decision Trees.  </li>
        <li>
            different segments should differ on risk-based performance and need to translate into measurable effects on business
            e.g. lower losses or higher approval rate
        </li>
        <li> should take care of future strategies and future target markets </li>
        <li> each segment should have sufficient volume </li>
        <li> More segments result in increase of
            <ul>
                <li> Development cost </li>
                <li> Implementation cost </li>
                <li> Processing cost </li>
                <li> Strategy Development and Monitoring complexity </li>
            </ul>
        </li>
    </ol><br>

    <p>
        <mark>
            Basel II defines segments as "Homogeneous Risk Pools".
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Segmentation > Experience-Based (Heuristic) Segmentation </strong></h4>
    <ol>
        <li> Segments with Higher Bad Rate  </li>
        <li> New subproduct </li>
        <li> Predefined group - Golden group  </li>
        <li> Future marketing direction </li>
        <li> Typical Experience based segments :
            <ul>
                <li> <strong> Demographics </strong> - Region, Age, Lifestyle, Time at Bureau, MOB </li>
                <li> <strong> Product type </strong> - Gold / Platinum cards, Mortgage duration, Insurance Type, Secured / Unsecured, Amount of Loan etc. </li>
                <li> <strong> Channel </strong> - Branch / Internet / Store-front / Dealers / Brokers etc. </li>
                <li>
                    <strong> Data Availability </strong> - Thick <mark> vs </mark> Thin (no trades) file,
                    Clean <mark> vs </mark> Dirty (-ve performance at Bureau), Revolver / Transactor etc.
                </li>
                <li> <strong> Applicant Type </strong> -
                        Existing <mark> vs </mark> New Customer,
                        First time home buyer <mark> vs </mark> second mortgage,
                        Profession etc.
                </li>
                <li> Product Ownership </li>
            </ul>
        </li>
    </ol><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Segmentation > Clustering </strong></h4>
    <ol>
        <li><code> Using K-Means clustering </code></li>
        <li>
            Compare Means / Distribution of variable in cluster to full population <code> normalized mean </code>  to
            come up with characteristic attributes of a cluster.
        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 2 > Data Review > Segmentation > Comparison </strong></h4>
    <ol>
        <li> Compare KS, C-statistics of generic model overlaid on segment population <mark> vs </mark> segmented model </li>
        <li>
            compare bad rate through generic model overlaid on segment population <mark> vs </mark> segmented model by
            fixing approval rate. Using this, an estimate on reduction in losses in terms of dollar value can be calculated.
        </li>
    </ol><br>



    <h4 class="bs-example"><strong> Stage 2 > Data Review > Methodology </strong></h4>
    <ol>
        <li> Data Quality: Decision tree is more appropriate if lot of missing data or
            non linear relationship between target and characteristics. </li>
        <li> Type of Target: Continuous or binary. </li>
        <li> sample size </li>
        <li> implemenation platform </li>
        <li> legal and compliance </li>
    </ol>


    <br><h3 class="page-header"> Stage 3 - Details</h3>


    <h4 class="bs-example"><strong> Stage 3 > Dev sample Creation > Specification </strong></h4>
    <ol>
        <li> No. of scorecards and specification of each segment </li>
        <li> definition of Bad, Good and Indeterminate </li>
        <li> portfolio bad rate and approval rate for each segment </li>
        <li> performance and sample windows </li>
        <li> definition of exclusions </li>
        <li> sample size required for each segment </li>
        <li> detailed list of variables </li>
        <li> derived variables </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 3 > Dev sample Creation > Variable Selection </strong></h4>
    <ol>
        <li> Expected predictive power based on experience </li>
        <li> Reliability and Robustness - e.g. Income, Application form data (optional fields), Occupation data (Interpretation) </li>
        <li> Human Intervention - e.g. Bankruptcy Indicator where Bankrupt accounts were declined due to policy cuts </li>
        <li> Legal / Compliance issue on some variables - Marital status, gender etc. </li>
        <li> Ratios - short term by long terms e.g. Inquiry, purchases, payment, utilization and  balance etc.</li>
        <li> Future Availability </li>
        <li> Changes in competitive environment, future direction / strategy  </li>
    </ol><br>


    <p>
        <mark>
            An increase in competitive environment will increase the average number of inquiries an applicant has at the bureau.
        </mark>
    </p><br>

    <p>
        <mark>
            For example, Over 4 inquiries in 12 months might change from high risk to medium risk performance over time and vice versa.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 3 > Dev sample Creation > Sampling </strong></h4>
    <ol>

    </ol><br>

    <p>
        <mark>
            Large enough sample reduces the impact of <code> multicollinearity </code> .
        </mark>
    </p><br>

    <p>
        <mark><code>
            Sample size calculations?? power test??
        </code></mark>
    </p><br>



    <h4 class="bs-example"><strong> Stage 3 > Dev sample Creation > Data Creation </strong></h4>
    <ol>
        <li> Oversampling </li>
        <li> Randomness check - sample is Representative of segment full population </li>
        <li> Aware of any data quirks </li>
    </ol>


    <br><h3 class="page-header"> Stage 4 - Details</h3>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Missing Values and Outliers </strong></h4>
    <ol>
        <li> Exclude all missing data - will result in very little data to work with </li>
        <li> Exclude attributes with high (e.g. more than 50%) missing values - particularly if trend is expected to continue in future </li>
        <li> Treat missing as separate group - impute based on business knowledge or insight into cause for missing value </li>
        <li> Statistical techniques - based on similar bad rate, Mean, Median or use variable as WOE. </li>
    </ol><br>

    <p>
        <mark>
            "Most frequent" and "mean" value imputation will cause spikes in data and differentiation between
            accounts with imputed value and actually having that value will be lost.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Clustering </strong></h4>
    <ol>
        <li> Proc varclus to reduce number of attributes and remove collinearity </li>
        <li> Can choose more than 1 variable from each cluster </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > WOE Transformation Benefits </strong></h4>
    <ol>
        <li> Easier to deal with outliers and missing etc. </li>
        <li> Easier to understand relationship </li>
        <li> Nonlinear variables can be used </li>
        <li> more control over scorecard composition by shaping the groups </li>
        <li> less over fitting and more elasticity to withstand some population change </li>
        <li> easier buy-in from business </li>
    </ol><br>


    <p>
        <mark>
            Over generalization concern of using WOE can be tackled with using more characteristics in the model.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Variable Strength </strong></h4>
    <ol>
        <li> WOE Trend </li>
        <li> IV </li>
        <li> Operational and business considerations </li>
    </ol><br>


    <p>
        <mark>
            WOE measures the strength of each attribute in separating good and bad accounts. it is a measure of
            the difference between proportions of goods and bads and each attribute.
        </mark>
    </p><br>

    <p>
        <mark>
            During WOE transformation, reversals may be reflecting actual behavior or data and masking them can
            reduce the overall strength of the attribute.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > IV </strong></h4>
    <ol>
        <li> Less than 0.02 : un-predictive </li>
        <li> 0.02 to 0.1 : weak </li>
        <li> 0.1 to 0.3 : medium </li>
        <li> 0.3 + : strong </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Preliminary Scorecard </strong></h4>
    <ol> A Credit card scorecard should have
        <li> Demographic data: Age, Residential status, region or time of employment </li>
        <li> Bureau Data: Tenure, inquiries, trades or payment performance etc. </li>
        <li> debt related variables </li>
        <li> internal performance data for existing customers </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Logistic Regression </strong></h4>
    <ol>
        <li> Maximum likelihood is used to estimate parameters (Betas) </li>
        <li> These parameter estimates measure the rate to change of logit for one unit change in input variable. </li>
        <li> Example of full risk profile:
            <ul>
                <li> Age </li>
                <li> Residential status </li>
                <li> Postal code </li>
                <li> Time in industry </li>
                <li> Time at address </li>
                <li> Inquiries 3 months / Inquiries 12 months </li>
                <li> Inquiries 12 months / Trades opened 12 months </li>
                <li> Trades 90 days+ as % of total </li>
                <li> Trades opened last 3 months / Trades opened last 12 months </li>
                <li> Utilization </li>
                <li> Number of products at bank </li>
                <li> Delinquency at bank </li>
                <li> Total debt service ratio </li>
            </ul>
        </li>
        <li> start from business justified weaker characteristics and iteratively add stronger characteristics. </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Reject Inference </strong></h4>
    <ol>
        <li> very high or very low confidence in previous approval criterion reduces the need of reject inference. </li>
        <li> reject inference is more than just an exercise to comply with statistical principles. </li>
        <li> swapset is the exchange of known bads with inferred goods </li>
        <li> <strong> Techniques: </strong>
            <ul>
                <li> Assign all rejects to BAD - high confidence in current approval process and high approval rate </li>
                <li> Assign rejects in same proportions of Good to Bad as Accepts - current approval process is random </li>
                <li> Ignore the Rejects -
                    <ul>
                        <li> Score TTD population with current approval scorecard</li>
                        <li> Score the Accepts with new scorecard and decline accounts below cut off </li>
                    </ul>
                </li><br>

                <li> Approve all applications and collect data for a short period of time -
                    <ul>
                        <li> can use bad rate to minimize the approvals </li>
                        <li> Assign lower credit lines to those below cut offs to minimize losses </li>
                        <li> can collect data over few months to avoid seasonality </li>
                        <li> may present a challenge with legal and compliance </li>
                    </ul>
                </li><br>

                <li> Bureau Data based method
                    <ul>
                        <li> Rejects are tagged good or bad based on their performance on a similar product with other
                            issuer opened in similar time frame.
                        </li>
                        <li> may present data collection challenges based on regulations </li>
                        <li> same definition of good/bad need to be used as Accepts but using different data sources </li>
                        <li> Applicants declined at one institution or for one product and likely to be declined elsewhere </li>
                    </ul>
                </li><br>

                <li> Based on Characteristics - Clustering
                    <ul>
                        <li> Make clusters of TTD population with bureau data </li>
                        <li> Assign same odds as Accepts in a cluster to Rejects </li>
                    </ul>
                </li><br>


                <li> Simple Augmentation
                    <ul>
                        <li> Score Rejects using Good/Bad model built on Accepts and get expected bad rate, p(bad) </li>
                        <li> select a cutoff (based on bad rate / prob on Accepts) and label Rejects as Good/Bad </li>
                        <li> this method does not take into account the prob of Rejects being approved </li>
                    </ul>
                </li><br>


                <li> Augmentation 2
                    <ul>
                        <li> Develop an Accept / Reject model </li>
                        <li> Assign p(Accept) to Accepts data as weights </li>
                        <li> Build a Good / Bad model on Accepts with weights and then follow same steps as simple augmentation </li>
                    </ul>
                </li><br>


                <li> Parceling
                    <ul>
                        <li> Score Rejects using Good/Bad model built on Accepts </li>
                        <li> Assign same proportion of good/bad as Accepts in same score bands </li>
                        <li> Assignment of good/bad classes can be random </li>
                        <li> since proportion of good/bad in Rejects should be higher than that of Accepts, a factor
                        can be used </li>
                    </ul>
                </li><br>

                <li> Fuzzy Augmentation
                    <ul>
                        <li> Score Rejects using Good/Bad model built on Accepts </li>
                        <li> duplicate Reject accounts with weight p(bad) and weight p(good) </li>
                        <li> Develop an Accept / Reject model </li>
                        <li> Assign further weight p(accept) to each observation in duplicated dataset in point 2 </li>
                    </ul>
                </li>
            </ul>
        </li>
    </ol><br>

    <p>
        <mark>
            If the previous credit granting was good and the approval rate was also high then inferred rejects should
            have a bad rate of atleast 3 or 4 times that of approves.
        </mark>
    </p><br>

    <p>
        <mark>
            A medium approval rate may yield an inferred bad rate of only twice that of approves.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Final Scorecard </strong></h4>
    <ol>
        <li> Using "All Good Bad data", rebuild the score card </li>
        <li> should not limit to variables selected in preliminary phase. </li>
        <li> In ideal case, all the analysis done in preliminary scorecard phase need to be repeated </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Scaling </strong></h4>
    <ol>
        <li> Score = Offset + Factor * ln(odds) </li>
        <li> score + pdo = offset + factor * ln(2*odds) </li>
        <li> factor = pdo / ln(2) </li>
        <li> offset = score - {factor * ln(odds) </li>
        <li> suppose a scorecard was being scaled so as to have odds of 50:1 at 600 points and odds to double every 20 points,
        we can calculate offset and factor using above equations. </li>
    </ol><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Confusion Matrix </strong></h4>
    <table class="table table-bordered table-condensed text-center">
        <tr>
            <td>  </td>
            <td colspan = "2"><strong> Predicted </strong></td>
        </tr>
        <tr>
            <td><strong> Actual </strong></td>
            <td><code> Good </code></td>
            <td><code> Bad </code></td>
        </tr>
        <tr>
            <td><code> Good </code></td>
            <td> True Positive (Acceptance of goods) </td>
            <td> False Negative (Decline goods) - Type 2 error </td>
        </tr>
        <tr>
            <td><code> Bad </code></td>
            <td> False Positive (Acceptance of bads) - Type 1 error </td>
            <td> True Negative (Decline bads) </td>
        </tr>
    </table>

    <ol>
        <li> Accuracy: (true positives and negatives) / (total cases) </li>
        <li> Error rate: (false positives and negatives) / (total cases) </li>
        <li> Sensitivity: (true positives) / (total actual positives) </li>
        <li> Specificity: (true negatives) / (total actual negatives) </li>
    </ol><br>


    <p>
        <mark>
            if objective is to reduce losses
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            choose scorecard with maximum specificity
        </mark>
    </p><br>

    <p>
        <mark>
            if objective is to increase market share
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            choose scorecard with maximum sensitivity
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Akaike Information Criterion </strong></h4>
    <ol>
        <li> AIC = N * ln(SSerror/N) + 2K where K = #parameters + 1 </li>
        <li> Model with lower AIC has better fit. </li>
    </ol><br>


    <table class="table table-bordered table-condensed text-center">
        <tr>
            <td>  </td>
            <td><strong> N </strong></td>
            <td><strong> K </strong></td>
            <td><strong> SS error </strong></td>
            <td><strong> AIC </strong></td>
        </tr>
        <tr>
            <td> Linear </td>
            <td> 20 </td>
            <td> 3 </td>
            <td> 122 </td>
            <td><code> 156.17  </code></td>
        </tr>
        <tr>
            <td> Polynomial </td>
            <td> 20 </td>
            <td> 4 </td>
            <td> 89 </td>
            <td><code> 189.86 </code></td>
        </tr>
    </table>


    <p>
        <mark>
            Sensitivity
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            cummulative % Good capture
        </mark>
    </p><br>

    <p>
        <mark>
            Specificity
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            cummulative % Bad capture
        </mark>
    </p><br>

    <p>
        <mark>
            AUC
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            Area under ROC Curve (sensitivity on y-axis vs specificity on x-axis) = c-statistic
        </mark>
    </p><br>

    <p>
        <mark>
            Gini (measure of inequality, Gini = 0 means perfect equality)
            <icon class="glyphicon glyphicon-arrow-right"></icon>
            2 * AUC (Lorenz curve) - 1 (equals double the area of half moon)
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Stage 4 > Scorecard Development > Comparing Scorecards </strong></h4>
    <ol>
        <li> Model with better KS at the approval rate or bad rate cut-off is better </li>
        <li> In general, performance with respect to any metric should be compared at approval rate / bad rate cutoff </li>
        <li> though for bankruptcy / response models, it makes sense to look at lowest decile since the objective
            is to isolate worst/best few performers for action. </li>
    </ol><br>

    <p>
        <mark>
            Expected loss = EAD x PD x LGD
        </mark>
    </p><br>


    <br><h3 class="page-header"> Basel Accords </h3>

    <p>
        Basel Accords refers to recommendations on banking regulations issued by Banking committee on Banking supervision.
    </p><br>

    <h4 class="bs-example"><strong> Basel 1 </strong></h4>
    <ol>
        <li>
            The first accord was the Basel I. It was issued in 1988 and focused mainly on credit risk by creating a bank
            asset classification system. This classification system grouped a bank's assets into five risk categories:
            <ul>
                <li> 0% - cash, bullion, home country debt like Treasuries </li>
                <li> 0%, 10%, 20% or 50% - public sector debt </li>
                <li> 20% - development bank debt, cash in collection </li>
                <li> 50% - residential mortgages </li>
                <li>
                    100% - private sector debt, real estate, plant and equipment, capital instruments issued at other banks,
                    education loan (RBI)
                </li>
            </ul>
        </li><br>

        <li>
            Banks with an international presence are required to hold capital equal to 8% of their risk-weighted assets (RWA)
            <ul>
                <li> At least, 4% in Tier I Capital (Equity Capital, retained earnings, reserves) </li>
                <li> More than 8% in Tier I and Tier II Capital </li>
                <li> Tier 1 capital - Permanent capital, Tier 2 capital - Supplementary capital </li>
            </ul>
        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 2 </strong></h4>
    <ol>
        <li>
            Aims to determine how much capital banks should have in place for the types of Risk they face
            in their lending and investment activities.
        </li>
        <li> published in 2004 and implementation started in 2007 </li>
        <li> make regulatory capital more risk sensitive </li>
        <li> promote enhanced risk management practices </li>
        <li> improve consistency of bank capital requirement internationally </li>
        <li>
            Pillars
            <ul>
                <li> Minimum Capital Requirement </li>
                <li> Supervisory Review process </li>
                <li> Market Discipline </li>
            </ul>
        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 2 > Pillar 1 > Minimum Capital Requirement </strong></h4>
    <ol>
        <li> Credit Risk - Lending </li>
        <li> Market Risk - trading, forex etc.</li>
        <li> Operational Risk - Non Financial like natural disaster, theft, IT breakdown etc. </li>
        <li> capital requirement = Min 8% of (Credit RWA + Market RWA + Operational RWA), at least 4% from Tier 1 </li>
        <li>
            Weights by RBI
            <ul>
                <li> 0% - Cash </li>
                <li> 20% - Balance with other banks </li>
                <li> 50% - House Loan less than 30lacs </li>
                <li> 75% - Education, Retail, House loans more than 30 lacs </li>
                <li> 125% - Personal loans, capital market exposure </li>
                <li> 150% - Commercial real estate </li>
            </ul>
        </li>
    </ol><br>

    <p>
        <mark>
            Capital Risk reflects credit ratings of counter-parties
        </mark>
    </p><br>

    <p>
        <mark>
            Market risk based on 1996 amendment
        </mark>
    </p><br>

    <p>
        <mark>
            New capital charge for operational risk is introduced in Basel 2.
        </mark>
    </p><br>


    <p>
        <mark>
            Credit Risk calculation (RBI) has two approaches - Standard weights & Internal Rating based approach (IRBA).
        </mark>
    </p><br>

    <p>
        <mark>
            Operational Risk calculation (RBI) has two approaches - Basic Indicator (15% of average gross profit in 3 yrs)
            & standard approach.
        </mark>
    </p><br>


    <h4 class="bs-example"><strong> Basel 2 > Pillar 1 > Minimum Capital Requirement > Market Risk </strong></h4>
    <ol>
        <li> Trading Book Items - derivatives, forex, commodities, stocks, bonds etc. </li>
        <li> Off balance sheet items - Letter of credit, repurchase programs, interest rate swaps etc. </li>
        <li> Capital Requirement
            <ul>
                <li> (k * VaR) + SRC  - Internal model approach </li>
                <li>
                    VaR is Value at Risk, maximum estimated loss that bank can have over 10 day period with 99%
                    confidence interval. Used as greater of previous day VaR and average of last 60 days.
                </li>
                <li> SRC is specific Risk charge - capital charge of company specific Risk. </li>
            </ul>

        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 2 > Pillar 1 > Minimum Capital Requirement > Capital Components </strong></h4>
    <ol>
        <li> Tier 1 : Permanent capital - Stocks / Common Equity, Reserves </li>
        <li> Tier 2 : Supplementary capital - Long term debt, cumulative preferred stock </li>
        <li> At least 4% should be from Tier 1 </li>
        <li> RBI requires minimum 9% capital of RWA, of which 6% should be from Tier 1 capital </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 2 > Pillar 2 > Supervisory Review </strong></h4>
    <p> Allows regulators the discretion to consider local conditions in their implementation of Basel rules. 4 key principles: </p>
    <ol>
        <li> Banks should have a process for assessing their capital requirement based on their risk profile. </li>
        <li> Regulators should review and evaluate bank's assessment and strategies. </li>
        <li> Banks are expected to operate above minimum regulatory standards. </li>
        <li> Regulators should intervene at an early stage to prevent capital from falling below min levels. </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 2 > Pillar 3 > Market Discipline </strong></h4>
    <p> Requires banks to fully disclose their risk assessment procedures and capital adequacy. Examples: </p>
    <ol>
        <li> Total Tier 1 and Tier 2 capital. </li>
        <li> List / Details of instruments in Tier 1. </li>
        <li> Capital Requirement of Credit Risk, Market Risk and Operational Risk </li>
        <li> Design of Risk management activities. </li>
    </ol><br>

    <br>
    <h4 class="bs-example"><strong> Basel 3 </strong></h4>
    <ol>
        <li> Issued in Sep 12 2010, substantial strengthening of existing capital requirements.  </li>
        <li> Addresses deficiencies in financial regulations revealed by 2008 global financial crisis. </li>
        <li> New standards on capital adequacy and Liquidity </li>
        <li> Minimum capital requirements
            <ul>
                <li>
                    <strong> Tier 1 Capital Ratio </strong>
                    <ul>
                        <li> Tier 1 capital ratio: increased from 4% in Basel 2 to 6% in Basel 3  </li>
                        <li> Core Tier 1 capital (common equity) ratio : increased from 2% in Basel 2 to 4.5% in Basel 3 </li>
                    </ul>
                </li>

                <li>
                    <strong> Capital conservation Buffer </strong>
                    <ul>
                        <li> 2.5% in core tier 1 capital to withstand future period of stress  </li>
                    </ul>
                </li>

                <li>
                    <strong> Counter-cyclical Capital Buffer </strong>
                    <ul>
                        <li> 0% to 2.5% in tier 1 capital during periods of high credit growth  </li>
                    </ul>
                </li>

                <li>
                    <strong> Capital for Global Systemically Important banks </strong>
                    <ul>
                        <li> should have loss absorbing capacity beyond above standards.  </li>
                    </ul>
                </li>
            </ul>
        </li>
    </ol><br>


    <h4 class="bs-example"><strong> Basel 3 - Additional Requirements </strong></h4>
    <ol>
        <li> Min 3% Leverage Ratio (Tier 1 capital by bank's average consolidated assets) </li>
        <li>
            Liquidity:
            <ul>
                <li>
                    <strong> Liquidity Coverage Ratio </strong>
                    - To ensure that banks have sufficient high quality liquid assets (cash or near cash) to cover
                    total net cash flows over 30 days
                </li>

                <li>
                    <strong> Net Stable funding ratio </strong>
                    - To ensure that banks maintain sufficient long term, stable sources of funding (customer deposits,
                    long term loans and equity) to cover their long term assets.
                    Ratio is defined as % of long term assets funded by long term stable funding.
                </li>
            </ul>
        </li>
    </ol>



    <br><h3 class="page-header"> VaR </h3>
    <ol>
        <li> Historical data plot - will follow normal distribution, maximum loss point in 99% of days </li>
        <li> Normal probability distribution - calculate std deviation, loss at point (2.575 * sigma) </li>
    </ol>


    <br><h3 class="page-header"> Multicollinearity </h3>
    <ol>
        <li> Two independent variables x1 and x2 are collinear when they are correlated with each other  </li>
        <li> in multiple regression study, we assume that all X variable are independent of each other </li>
        <li> also assumption is that each X variable has a unique piece of information about Y. </li>

        <br><li>
            Y = b0 + b1x1 + b2x2 + c
            <ul>
                <li> b1 is the change in Y for 1 unit change in x1 while x2 held constant </li>
                <li> similarly b2 is the change in Y for 1 unit change in x2 while x1 held constant </li>
                <li> However when multicollinearity exists, b1 and b2 won't be estimated accurately </li>
            </ul>
        </li><br>

        <li>
            Impact
            <ul>
                <li> standard error of coefficients increases </li>
                <li> magnitude of coeff may not be accurate </li>
                <li> signs of coeff may change </li>
            </ul>
        </li>
    </ol>

    <p>
        <mark>
            Caveat: Even though corr coeff between any two X variables may be small but 3 independent variables
            x1, x2 and x3 may be highly correlated as a group.
        </mark>
    </p>


    <br><h3 class="page-header"> VIF </h3>
    <ol>
        <li> vif = 1 / ( 1 - r-square) </li>
        <li> where r-squre is calculated by regressing one variable against rest </li>
        <li> VIF of 10 means that var(bi) is 10 times of what it should be if there was no collinearity existed. </li>
    </ol>


    <br><h3 class="page-header"> Normal Distribution </h3>
    <ol>
        <li> f(x) = (1 / (root 2 pi) * sigma) * exp ( -0.5 * (x - u / sigma) squared </li>
        <li>  mean = median = mode </li>
        <li>  68% of values lies within 1 sigma, 95.5% in 2 sigma and 99.7% in 3 sigma </li>
    </ol>

    <p>
        <mark>
            Binomial variable : mean = np, variance = npq
        </mark>
    </p>

    <p>
        <mark>
            Poisson variable : mean = np, variance = np
        </mark>
    </p>


    <br><h3 class="page-header"> Central Limit Theorem </h3>
    <p>
        Arithmetic Mean (scaled by root n) of n independently and identically distributed random variables with some arbitrary distribution,
        0 mean and variance sigma2 will approximately follow normal distribution as n increases.
    </p>


    <br><h3 class="page-header"> PSI </h3>
    <p>
        psi = summation ( (% exp - % obs) * ln(% exp / % obs) )
    </p>


    <br><h3 class="page-header"> Hosmer Lemeshow </h3>
    <p>
        h = summation ( (obs events - exp events)2 / N * event rate * (1 - event rate)   )
    </p>

    <br><h3 class="page-header"> Logistic Regression </h3>
    <ol>
        <li> dependent and independent variable does not need to be linearly related  </li>
        <li> independent variables do not need to be multivariate normal </li>
        <li> error terms do not need to be multivariate normally distributed </li>
        <li> homoscedasticity is not needed. Logistic regression does not need variances to be heteroscedastic for
            each level of the independent variables </li>
        <li> the error terms need to be independent </li>
        <li> Logistic regression requires each observation to be independent </li>
        <li> the independent variables should be independent from each other </li>
        <li> logistic regression assumes linearity of independent variables and log odds </li>
        <li> it requires quite large sample sizes, statisticians recommend at least 30 cases for each parameter to be estimated</li>
    </ol><br>


</div>

<footer style=" text-align:center; padding-top:40px">
    <br>
    <br>
</footer>



</body></html>